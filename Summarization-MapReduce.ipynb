{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers --quiet\n",
        "!pip install langchain\n",
        "!pip install -U sentence-transformers\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "WDHy1boQZ1xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "6nQ92ndx2wSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import difflib\n",
        "import re\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "J2jk_TqA06a_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the model"
      ],
      "metadata": {
        "id": "BYGpg7nl25SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"pszemraj/long-t5-tglobal-base-16384-book-summary\")\n",
        "\n",
        "map_chain = pipeline(\n",
        "        task=\"summarization\",\n",
        "        model=\"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=300,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "veFG4NJ40VRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getbook(bookname):\n",
        "        # Function to search for a book by name and return the best match URL\n",
        "        def search_book_by_name(book_name):\n",
        "            base_url = \"https://www.gutenberg.org/\"\n",
        "            search_url = base_url + \"ebooks/search/?query=\" + book_name.replace(\" \", \"+\") + \"&submit_search=Go%21\"\n",
        "\n",
        "            response = requests.get(search_url)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            # Find the best match link based on similarity ratio\n",
        "            best_match_ratio = 0\n",
        "            best_match_url = \"\"\n",
        "\n",
        "            for link in soup.find_all(\"li\", class_=\"booklink\"):\n",
        "                link_title = link.find(\"span\", class_=\"title\").get_text()\n",
        "                similarity_ratio = difflib.SequenceMatcher(None, book_name.lower(), link_title.lower()).ratio()\n",
        "                if similarity_ratio > best_match_ratio:\n",
        "                    best_match_ratio = similarity_ratio\n",
        "                    best_match_url = base_url + link.find(\"a\").get(\"href\")\n",
        "\n",
        "            return best_match_url\n",
        "\n",
        "        # Function to get the \"Plain Text UTF-8\" download link from the book page\n",
        "        def get_plain_text_link(book_url):\n",
        "            response = requests.get(book_url)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            plain_text_link = \"\"\n",
        "\n",
        "            for row in soup.find_all(\"tr\"):\n",
        "                format_cell = row.find(\"td\", class_=\"unpadded icon_save\")\n",
        "                if format_cell and \"Plain Text UTF-8\" in format_cell.get_text():\n",
        "                    plain_text_link = format_cell.find(\"a\").get(\"href\")\n",
        "                    break\n",
        "\n",
        "            return plain_text_link\n",
        "\n",
        "\n",
        "        # Function to get the content of the \"Plain Text UTF-8\" link\n",
        "        def get_plain_text_content(plain_text_link):\n",
        "            response = requests.get(plain_text_link)\n",
        "            content = response.text\n",
        "            return content\n",
        "\n",
        "\n",
        "\n",
        "        # book_name = input(\"Enter the name of the book: \")\n",
        "        book_name = bookname\n",
        "        best_match_url = search_book_by_name(book_name)\n",
        "\n",
        "        if best_match_url:\n",
        "            plain_text_link = get_plain_text_link(best_match_url)\n",
        "            if plain_text_link:\n",
        "                full_plain_text_link = \"https://www.gutenberg.org\" + plain_text_link\n",
        "                plain_text_content = get_plain_text_content(full_plain_text_link)\n",
        "                # unstring \"plain_text_content\" to print the whole book\n",
        "        #         print(\"Plain Text UTF-8 content:\", plain_text_content)\n",
        "                book_text = plain_text_content\n",
        "            else:\n",
        "                print(\"No Plain Text UTF-8 link found.\")\n",
        "        else:\n",
        "            print(\"No matching book found.\")\n",
        "\n",
        "        return book_text\n",
        "\n",
        "\n",
        "# Tested book names\n",
        "#     -The changed brides\n",
        "#     -The bride's fate\n",
        "#     -Jane Eyre: An Autobiography by Charlotte BrontÃ«"
      ],
      "metadata": {
        "id": "GhiAgJ_A6vih"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(book_text):\n",
        "\n",
        "          cleaned_text = book_text.replace('\\r', '').replace('\\n', '')\n",
        "          cleaned_text = re.sub(r'\\\\u[a-fA-F0-9]+', '', cleaned_text)\n",
        "\n",
        "          num_tokens = tokenizer.encode(cleaned_text, add_special_tokens=True)\n",
        "\n",
        "          text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "          chunks = text_splitter.create_documents([cleaned_text])\n",
        "\n",
        "          embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
        "          vectors = embeddings.embed_documents([x.page_content for x in chunks])\n",
        "\n",
        "          num_clusters = 10\n",
        "          # Perform K-means clustering\n",
        "          kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)\n",
        "          # Filter out FutureWarnings\n",
        "          simplefilter(action='ignore', category=FutureWarning)\n",
        "          # Perform t-SNE and reduce to 2 dimensions\n",
        "          tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
        "          vectors_array = np.array(vectors)\n",
        "          reduced_data_tsne = tsne.fit_transform(vectors_array)\n",
        "\n",
        "          closest_indices = []\n",
        "          # Loop through the number of clusters you have\n",
        "          for i in range(num_clusters):\n",
        "              # Get the list of distances from that particular cluster center\n",
        "              distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "              # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "              closest_index = np.argmin(distances)\n",
        "              # Append that position to your closest indices list\n",
        "              closest_indices.append(closest_index)\n",
        "\n",
        "          selected_indices = sorted(closest_indices)\n",
        "          selected_docs = [chunks[doc] for doc in selected_indices]\n",
        "\n",
        "          return selected_docs\n",
        "\n",
        "def genSummary(docs):\n",
        "\n",
        "    selected_docs = docs\n",
        "    summary_list = []\n",
        "    # Loop through a range of the lenght of your selected docs\n",
        "    for i, doc in enumerate(selected_docs):\n",
        "            # Go get a summary of the chunk\n",
        "            # chunk_summary = map_chain.run([doc])\n",
        "            chunk_summary = map_chain(\n",
        "                    doc.page_content,\n",
        "                    min_length=100,\n",
        "                    max_length=200,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    encoder_no_repeat_ngram_size=3,\n",
        "                    repetition_penalty=3.5,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True,\n",
        "                )\n",
        "            summary_list.append(chunk_summary)\n",
        "            # print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:250]} \\n\")\n",
        "\n",
        "\n",
        "    summary_list2 = []\n",
        "    for i in range(len(summary_list)):\n",
        "         summary_list2.append(summary_list[i][0][\"summary_text\"])\n",
        "\n",
        "    summaries = \"\\n\".join(summary_list2)\n",
        "\n",
        "    # Convert it back to a document\n",
        "    summaries = Document(page_content=summaries)\n",
        "    num_tokens_sum = tokenizer.encode(summaries.page_content, add_special_tokens=True)\n",
        "    output = map_chain(\n",
        "            summaries.page_content,\n",
        "            min_length=100,\n",
        "            max_length=200,\n",
        "        )\n",
        "\n",
        "    print(output)\n",
        "    print(output[0][\"summary_text\"])\n",
        "    output[0][\"summary_text\"]"
      ],
      "metadata": {
        "id": "xMpSH0m00PA3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "booktxt = getbook(\"The changed brides\")\n",
        "selec_Docs = cleanText(booktxt)\n",
        "sum = genSummary(selec_Docs)"
      ],
      "metadata": {
        "id": "ZvQSGmxEebk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "RdLJzDKUg9Fz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "d6dCuvwjhdXh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"GutenbergChat Summarization\"\n",
        "\n",
        "# Summary\n",
        "def get_summary(book_name):\n",
        "    if not book_name:\n",
        "        return \"Please enter the name of the book.\"\n",
        "\n",
        "    # load book\n",
        "    booktxt = getbook(book_name)\n",
        "    selec_Docs = cleanText(booktxt)\n",
        "    sum = genSummary(selec_Docs)\n",
        "\n",
        "    return f\"Book Summary: {book_summary}\\n\"\n",
        "\n",
        "summaryBot = gr.Interface(fn=get_summary, inputs=\"text\", outputs=\"text\", title=title + \" - Summary\")\n",
        "demo2 = gr.TabbedInterface([summaryBot], [\"Summary\"])\n",
        "demo2.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "jSN8pekur9pR",
        "outputId": "5733a179-7a88-48d6-f756-62dd870ea7a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://76e57fc59a83bca90a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://76e57fc59a83bca90a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}